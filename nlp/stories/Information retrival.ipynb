{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information retrival\n",
    "this notebook is based on the Scott's great [tutorial](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import numpy as np\n",
    "import num2words\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [x[0] for x in os.walk(str(os.getcwd())+ '/stories/')]\n",
    "dataset = []\n",
    "for folder in folders:\n",
    "    text = \"\"\n",
    "    with open(folder + '/index.html', 'r') as file:\n",
    "        text = file.read().strip()\n",
    "    urls = re.findall(r'><A HREF=\"(.*)\">.*</A> ', text)\n",
    "    titles = re.findall(r'<BR><TD> (.*)\\n', text)\n",
    "    assert len(urls) == len(titles)\n",
    "    dataset += list(zip(titles,  [folder + '\\\\' + u for u in urls] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in the dataset: 467\n"
     ]
    }
   ],
   "source": [
    "print('Total items in the dataset:', len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. Convert to lowercase\n",
    "2. Remove punctuations\n",
    "3. Remove stopwords\n",
    "4. Convert numbers into words\n",
    "5. Remove signle charachters\n",
    "6. Stemming/Lemmitzing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower_case(text):\n",
    "    return str(np.char.lower(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punc in string.punctuation:            \n",
    "        text = str(np.char.replace(text, punc, ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text_tokenized):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return [word for word in text_tokenized if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_char(text_tokenized):\n",
    "    return [word for word in text_tokenized if len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_2_str(text_tokenized):\n",
    "    result = []\n",
    "    for word in text_tokenized:\n",
    "        try:\n",
    "            value = num2words.num2words(float(word))\n",
    "        except:\n",
    "            value = word\n",
    "        result.append(value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmify(text_tokenized):\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in text_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipleline(text):\n",
    "    text = to_lower_case(text)\n",
    "    text_tokenized = word_tokenize(text)\n",
    "    text_tokenized = num_2_str(text_tokenized)\n",
    "    text_tokenized = word_tokenize(remove_punctuations(\" \".join(text_tokenized)))\n",
    "    text_tokenized = stemmify(text_tokenized)\n",
    "    text_tokenized = remove_single_char(text_tokenized)\n",
    "    text_tokenized = remove_stopwords(text_tokenized)\n",
    "    text = remove_punctuations(\" \".join(text_tokenized))\n",
    "    return word_tokenize(text) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stories_df = pd.DataFrame(columns=['title', 'body'])\n",
    "titles = []\n",
    "bodies = []\n",
    "for title, fp in dataset:\n",
    "    with open(fp, 'r', encoding='utf8', errors='ignore') as file:\n",
    "        text = file.read().strip()\n",
    "        titles.append(preprocessing_pipleline(title))\n",
    "        bodies.append(preprocessing_pipleline(text))\n",
    "    #stories_df.loc[len(stories_df)] = [title, text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = {}\n",
    "for i,body in enumerate(bodies):\n",
    "    for word in body:\n",
    "        if word in DF:\n",
    "            DF[word].add(i)\n",
    "        else:\n",
    "            DF[word] = {i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = DF.keys()\n",
    "N = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 38847\n"
     ]
    }
   ],
   "source": [
    "print('Total vocabulary:', N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_bag = {} # word -> docs -> score\n",
    "alpha = 0.6\n",
    "for idx, (title, body) in enumerate(zip(titles, bodies)):\n",
    "    word_count = Counter(title + body)\n",
    "    total_words = sum(word_count.values())\n",
    "    \n",
    "    for word in np.unique(body + title):\n",
    "        tf = word_count[word] / total_words\n",
    "        df = DF.get(word, 0)\n",
    "        idf = np.log(N / (df + 1) )\n",
    "        tf_idf_score = tf * idf\n",
    "        word_docs = tf_idf_bag.get(word, dict())\n",
    "        if word in title:\n",
    "            word_docs[idx] = tf_idf_score * alpha\n",
    "        else:\n",
    "            word_docs[idx] = tf_idf_score * (1 - alpha)\n",
    "        tf_idf_bag[word] = word_docs\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score(qry):\n",
    "    collected_weights = Counter()\n",
    "    collected_words = set()\n",
    "    ppsed_qry = preprocessing_pipleline(qry)\n",
    "    for word in ppsed_qry:\n",
    "        if word in tf_idf_bag:\n",
    "            collected_words.add(word)\n",
    "            available_docs_ids =  tf_idf_bag.get(word, {})\n",
    "            collected_weights += Counter(available_docs_ids)\n",
    "    \n",
    "    return sorted(collected_weights.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{212: 0.002232097342053498,\n",
       " 320: 0.0010257556197122708,\n",
       " 394: 0.0006081097502155977,\n",
       " 310: 0.00017746052557364558,\n",
       " 26: 0.00016731795611433966,\n",
       " 225: 0.00011716747845862313}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry = \"\"\"twitter\"\"\"\n",
    "dict(matching_score(qry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Great Learning: The Text of Confucious',\n",
       " 'E:\\\\Programming\\\\workbench\\\\python\\\\nlp\\\\stories/stories/\\\\greatlrn.leg')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[212]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 1, 'c': 3, 'd': 94})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter({'a': 1, 'b':0, 'c':3, 'd':94}) + Counter({}) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
